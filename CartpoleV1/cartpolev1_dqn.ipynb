{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with Keras for Open AI gym Carpole V1\n",
    "\n",
    "This is a simple implementation using a Deep Q Network as a function approximator for the Q values of the continuous state space of the cartpole v1 of Open AI gym. This task is more difficult and unstable than Cartpole-V0, so we will need techniques like Double Q Learning, experience replay (maybe prioritized), augmented state representation (using the \n",
    "\n",
    "Let's start by importing what is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "import sys\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from collections import deque\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables \n",
    "\n",
    "I will be using experience replay but instead of having a single buffer for memory I will be using a slightly different method. I will run the environment until there are at least 500 timesteps of experience (let's call them sessions). I will then append the session to a memory of sessions and will sample in a random order from this memory for experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SESSIONS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class\n",
    "Below is the class for the dqn agent. There is an option for using Double Q learning or simple Q learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class deepagent:\n",
    "    def __init__(self, env): \n",
    "        self.maxlen = 500 # memory size for experience replay\n",
    "        self.gamma = 0.95 # gamma for r + gamma*Q\n",
    "        self.mem = [] # initialization of memory buffer\n",
    "        self.epsilon = 1.0 # initial epsilon value for exploration\n",
    "        self.decay = 0.98 # decay of epsilon after every experience replay\n",
    "        self.lr = 0.0001 # learning rate for the weight updates\n",
    "        self.batch_size = 32 # batch size for each epoch of weight updates\n",
    "        self.C = 1 # after how many experience replays from mem buffer to update target\n",
    "        self.model = self.make_model() # initialize model with keras\n",
    "\n",
    "    def make_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = 4, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros', activation='relu'))\n",
    "        model.add(Dense(24, kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros', activation=\"relu\"))\n",
    "        model.add(Dense(12,kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros', activation=\"relu\"))\n",
    "        model.add(Dense(2,  kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros', activation='linear'))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if random.random()<self.epsilon: \n",
    "            return random.randrange(2) #return random action with prob epsilon\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(np.array(state).reshape(1,4)))\n",
    "\n",
    "    def update(self):\n",
    "        self.epsilon = max(self.decay*self.epsilon, 0.01) # decay epsilon up to 0.1\n",
    "\n",
    "    def expreplay(self, memory, size, target_agent):\n",
    "        memlength = len(memory) # the size of memory buffer for experience replay\n",
    "        iters = 5*memlength//size # sample \"size\" number of samples for \"iters\" times from mem \n",
    "        for i in range(iters):\n",
    "            print(\"learning from experience session: %03.2f %%\" %(100.0*i/(iters-1)), end='\\r')\n",
    "            weights = np.array(memory)[:,5].reshape(len(memory)) #\n",
    "            weights = np.array(weights, dtype='float64')\n",
    "            probs = weights/np.sum(weights) #probability for choosing each quadriple from mem\n",
    "            idx = np.array(range(len(weights)), dtype='int')\n",
    "            indices = np.random.choice(idx, size=size, replace=False, p=probs) #indices of chosen ones\n",
    "            data = np.array(memory)[indices,:].tolist() #minibatch of memory to replay\n",
    "            # data = np.array(random.sample(memory, size))\n",
    "            SS, TT = [] , []\n",
    "            for s, a, r, sp, done, td in data:\n",
    "                Q = self.model.predict(np.array(s).reshape(1,4))\n",
    "                Qsp = self.model.predict(np.array(sp).reshape(1,4))\n",
    "                Qhat = target_agent.model.predict(np.array(sp).reshape(1,4))\n",
    "                if done:\n",
    "                    Q[0][a] = r\n",
    "                else:\n",
    "                    # Q[0][a] = r + self.gamma*np.amax(Qhat) #Q-Learning\n",
    "                    Q[0][a] = r + self.gamma*Qhat[0][np.argmax(Qsp)] #Double Q-Learning\n",
    "                SS.append(s)\n",
    "                TT.append(Q.tolist()[0])\n",
    "            self.model.fit(np.array(SS), np.array(TT), epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent Function\n",
    "Below is the function to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traindqn(env):\n",
    "    agent = deepagent(env) #create agent\n",
    "    target_agent = deepagent(env) #create target agent\n",
    "    # agent.model.load_weights(\"./cartpolev1_dqn_weights.h5\")\n",
    "    # target_agent.model.load_weights(\"./cartpolev1_dqn_weights.h5\")\n",
    "    done = False\n",
    "    cumlist = [0]\n",
    "    session = 0\n",
    "    hypermem = []\n",
    "    while session<SESSIONS:\n",
    "        cumreward = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            cumreward += reward\n",
    "            reward = reward if not done else -10 #maybe not needed since it just shifts...\n",
    "            q = agent.model.predict(np.array(state).reshape(1,4))\n",
    "            qsp = agent.model.predict(np.array(next_state).reshape(1,4))\n",
    "            qhat = target_agent.model.predict(np.array(next_state).reshape(1,4))\n",
    "            if done:\n",
    "                td = abs(q[0][action] - reward)\n",
    "            else:\n",
    "                # td = abs(q[0][action] - reward - agent.gamma*np.amax(qhat))\n",
    "                td = abs(q[0][action] - reward - agent.gamma*qhat[0][np.argmax(qsp)]) #DoubleQ\n",
    "            agent.mem.append((state, action, reward, next_state, done, td))\n",
    "            state = cp.deepcopy(next_state)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        cumlist.append(0.0*cumlist[-1] + 1.0*cumreward)\n",
    "        print(\"%03.2f %%  \" % (100.0*session/SESSIONS), end=''),\n",
    "        print(\"epsilon: %03.3f \" %(agent.epsilon), end=''),\n",
    "        print(\"lr : %03.5f \" %(agent.lr), end=''),\n",
    "        print(\"memory: %03.3f \" %(len(agent.mem)), end=''),\n",
    "        print(\"reward: %03.3f \" %(cumlist[-1]), end='\\n')\n",
    "\n",
    "        if len(agent.mem)>=agent.maxlen:\n",
    "            hypermem.append(agent.mem)\n",
    "            if len(hypermem)>10:\n",
    "                hypermem.pop(random.randint(0,len(hypermem)-2)) #not the last session...\n",
    "            ses = 1\n",
    "            for hyp in np.random.permutation(range(len(hypermem))).tolist():\n",
    "                print(\"\")\n",
    "                print(\"running session: {}/{}\".format(ses, len(hypermem)))\n",
    "                agent.expreplay(hypermem[hyp], agent.batch_size, target_agent)\n",
    "                target_agent.model.set_weights(agent.model.get_weights()) #update target\n",
    "                ses+=1\n",
    "            agent.mem = [] #flush cache memory\n",
    "            if session%agent.C == 0:\n",
    "                target_agent.model.set_weights(agent.model.get_weights()) #update target\n",
    "            agent.update()\n",
    "            session+=1\n",
    "    agent.model.save_weights(\"./weights.h5\") #uncomment to save weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Agent Function\n",
    "Below is the function to test the agent after learning. I have alredy trained the agent for an architecture where I used three hidden layers of 24, 24, 12 units respectively, stored in \"cartpolev1_dqn_weights.h5\". This was the most stable version I could get this far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testdqn(env, trials, simulate):\n",
    "    agent = deepagent(env)\n",
    "    agent.model.load_weights(\"./cartpolev1_dqn_weights.h5\")\n",
    "    for test in range(trials):\n",
    "        agent.epsilon = 0.000\n",
    "        cumreward = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            if simulate:\n",
    "                env.render()\n",
    "            state = np.array(state).reshape(1, 4)\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            cumreward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        print(\"cumreward: \", cumreward)\n",
    "#         input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "Here I have already trained the network so I only call test function, but you can chose to also train it yourself. Take in mind that after 500 timesteps the simultator will end and the task will be considered successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 18:55:41,399] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumreward:  500.0\n",
      "cumreward:  500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# C = traindqn(env)\n",
    "# plt.plot(C)\n",
    "# plt.show()\n",
    "\n",
    "testdqn(env, trials=2, simulate=True) #choose number of experiments to try and if you want to observe the cartpole...\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
